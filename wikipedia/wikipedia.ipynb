{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2c0057d-3c7b-49ae-933e-fafc7e193cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib3\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cc270a-1512-447b-b5c8-10480b2f69ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "# How fast can we get to [philosophy](https://en.wikipedia.org/wiki/Philosophy) ?\n",
    "\n",
    "## Hypothesis\n",
    "In this experiment, I'll test the hypothesis that:\n",
    "**By going to the first link on any Wikipedia article, you'll end up on the [philosophy](https://en.wikipedia.org/wiki/Philosophy) article.** \n",
    "\n",
    "## Solution\n",
    "To do this, I simplified the problem to two smaller problems:\n",
    "- Getting links from article (parsing article)\n",
    "- Downloading article and building URL tree\n",
    "\n",
    "For each article I'll enter the first URL, if that URL contains the phrase`Philosophy` the algorithm will end.\n",
    "\n",
    "\n",
    "`get_links_from_wiki` function parses the article. It works by finding a div that contains the whole article, then iterates through all paragraphs and finds all links that match pattern `/wiki/article_name`. Because there's no domain in that pattern, it is added at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e1d066f-7404-4fe9-9035-e61d51541199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_from_wiki(soup, n=5, prefix=\"https://en.wikipedia.org\"):\n",
    "    \"\"\"\n",
    "    Extracts `n` first links from wikipedia articles and adds `prefix` to\n",
    "    internal links.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    soup : BeautifulSoup\n",
    "        Wikipedia page\n",
    "    n : int\n",
    "        Number of links to return\n",
    "    prefix : str, default=\"https://en.wikipedia.org\"\"\n",
    "        Site prefix\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of links\n",
    "    \"\"\"\n",
    "    arr = []\n",
    "    \n",
    "    # Get all paragraphs\n",
    "    for paragraph in soup.find(\"div\", class_=\"mw-parser-output\").find_all(\"p\"):\n",
    "        # In each paragraph find all <a href=\"/wiki/article_name\"></a> and extract \"/wiki/article_name\"\n",
    "        for i, a in enumerate(\n",
    "            paragraph.find_all(\"a\", href=True)\n",
    "        ):\n",
    "            if len(arr) >= n:\n",
    "                break\n",
    "            if a[\"href\"].startswith(\"/wiki\") and len(a[\"href\"].split(\"/\")) == 3 :\n",
    "                arr.append(prefix + a[\"href\"])\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fc3e2b-ccc2-43c5-83b8-19bd9eab899b",
   "metadata": {},
   "source": [
    "The crawl function will be recursive, for each URL found on page I'll call it again. For rach iteration it'll check if URL contains that phrase, if so it'll return both the site and link to Philosophy. To control number of recursive calls, depth of created tree is limited by `depth` parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f8ce581-3521-408b-9422-fe1e378f8870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl(\n",
    "    pool: urllib3.PoolManager,\n",
    "    url,\n",
    "    phrase=None,\n",
    "    deep=1,\n",
    "    sleep_time=0.5,\n",
    "    n=5,\n",
    "    prefix=\"https://en.wikipedia.org\",\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Crawls given Wikipedia `url` (article) with max depth `deep`. For each page\n",
    "    extracts `n` urls and  if `phrase` is given check if `phrase` in urls.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pool : urllib3.PoolManager\n",
    "        Request pool\n",
    "    phrase : str\n",
    "        Phrase to search for in urls.\n",
    "    url : str\n",
    "        Link to wikipedia article\n",
    "    deep : int\n",
    "        Depth of crawl\n",
    "    sleep_time : float\n",
    "        Sleep time between requests.\n",
    "    n : int\n",
    "        Number of links to return\n",
    "    prefix : str, default=\"https://en.wikipedia.org\"\"\n",
    "        Site prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Tuple of url, list\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        site = url.split(\"/\")[-1]\n",
    "        print(f\"{deep} Entering {site}\")\n",
    "\n",
    "    time.sleep(sleep_time)\n",
    "    site = pool.request(\"GET\", url)\n",
    "    soup = BeautifulSoup(site.data, parser=\"lxml\")\n",
    "    links = get_links_from_wiki(soup=soup, n=n, prefix=prefix)\n",
    "    is_phrase_present = any([phrase in link for link in links]) and phrase is not None\n",
    "    if deep > 0 and not is_phrase_present:\n",
    "        return (\n",
    "            url,\n",
    "            [\n",
    "                crawl(\n",
    "                    pool=pool,\n",
    "                    url=url_,\n",
    "                    phrase=phrase,\n",
    "                    deep=deep - 1,\n",
    "                    sleep_time=sleep_time,\n",
    "                    n=n,\n",
    "                    prefix=prefix,\n",
    "                    verbose=verbose,\n",
    "                )\n",
    "                for url_ in links\n",
    "            ],\n",
    "        )\n",
    "    return url, links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe1faa1-87b1-4a5b-b8d6-aec851d0af73",
   "metadata": {},
   "source": [
    "## The experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ea17ab0-ca9b-492d-9831-d14ee86506d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance of PoolManager that each crawler will share\n",
    "pool = urllib3.PoolManager() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea51c7d-9bfb-4251-afad-b77c49a4d405",
   "metadata": {},
   "source": [
    "To test the hypothesis we'll start from page `https://en.wikipedia.org/wiki/Data_mining\"`, look for page `Philosophy` and set link limit for crawler to `1` so that it'll only enter the first link on each page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de5c3d1c-089f-4b52-96c6-30261807a8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 Entering Data_mining\n",
      "29 Entering Data_set\n",
      "28 Entering Data\n",
      "27 Entering American_English\n",
      "26 Entering Variety_(linguistics)\n",
      "25 Entering Sociolinguistics\n",
      "24 Entering Society\n",
      "23 Entering Social_group\n",
      "22 Entering Social_science\n",
      "21 Entering Branches_of_science\n",
      "20 Entering Science\n",
      "19 Entering Latin_language\n",
      "18 Entering Classical_language\n",
      "17 Entering Language\n",
      "16 Entering Communication\n",
      "15 Entering Academic_discipline\n",
      "14 Entering Knowledge\n",
      "13 Entering Fact\n",
      "12 Entering Experience\n",
      "11 Entering Consciousness\n",
      "10 Entering Sentience\n",
      "9 Entering Emotion\n",
      "8 Entering Mental_state\n",
      "7 Entering Mind\n",
      "6 Entering Thought\n",
      "5 Entering Ideas\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('https://en.wikipedia.org/wiki/Data_mining',\n",
       " [('https://en.wikipedia.org/wiki/Data_set',\n",
       "   [('https://en.wikipedia.org/wiki/Data',\n",
       "     [('https://en.wikipedia.org/wiki/American_English',\n",
       "       [('https://en.wikipedia.org/wiki/Variety_(linguistics)',\n",
       "         [('https://en.wikipedia.org/wiki/Sociolinguistics',\n",
       "           [('https://en.wikipedia.org/wiki/Society',\n",
       "             [('https://en.wikipedia.org/wiki/Social_group',\n",
       "               [('https://en.wikipedia.org/wiki/Social_science',\n",
       "                 [('https://en.wikipedia.org/wiki/Branches_of_science',\n",
       "                   [('https://en.wikipedia.org/wiki/Science',\n",
       "                     [('https://en.wikipedia.org/wiki/Latin_language',\n",
       "                       [('https://en.wikipedia.org/wiki/Classical_language',\n",
       "                         [('https://en.wikipedia.org/wiki/Language',\n",
       "                           [('https://en.wikipedia.org/wiki/Communication',\n",
       "                             [('https://en.wikipedia.org/wiki/Academic_discipline',\n",
       "                               [('https://en.wikipedia.org/wiki/Knowledge',\n",
       "                                 [('https://en.wikipedia.org/wiki/Fact',\n",
       "                                   [('https://en.wikipedia.org/wiki/Experience',\n",
       "                                     [('https://en.wikipedia.org/wiki/Consciousness',\n",
       "                                       [('https://en.wikipedia.org/wiki/Sentience',\n",
       "                                         [('https://en.wikipedia.org/wiki/Emotion',\n",
       "                                           [('https://en.wikipedia.org/wiki/Mental_state',\n",
       "                                             [('https://en.wikipedia.org/wiki/Mind',\n",
       "                                               [('https://en.wikipedia.org/wiki/Thought',\n",
       "                                                 [('https://en.wikipedia.org/wiki/Ideas',\n",
       "                                                   ['https://en.wikipedia.org/wiki/Philosophy'])])])])])])])])])])])])])])])])])])])])])])])])])])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawl(pool, \"https://en.wikipedia.org/wiki/Data_mining\", phrase=\"Philosophy\", deep=30, n=1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217c59bc-3558-48c8-a160-44fe931b9ee4",
   "metadata": {},
   "source": [
    "As you can see after 25 iterations indeed we found `Philosophy` page."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
