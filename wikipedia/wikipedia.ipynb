{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2c0057d-3c7b-49ae-933e-fafc7e193cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib3\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cc270a-1512-447b-b5c8-10480b2f69ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "# How fast can we get to [philosophy](https://en.wikipedia.org/wiki/Philosophy) ?\n",
    "\n",
    "## Hypothesis\n",
    "In this experiment, I'll test the hypothesis that:\n",
    "**By going to the first link on any Wikipedia article, you'll end up on the [philosophy](https://en.wikipedia.org/wiki/Philosophy) article.** \n",
    "\n",
    "## Solution\n",
    "To do this, I simplified the problem to two smaller problems:\n",
    "- Getting links from article (parsing article)\n",
    "- Downloading article and building URL tree\n",
    "\n",
    "For each article I'll enter the first URL, if that URL contains the phrase`Philosophy` the algorithm will end.\n",
    "\n",
    "\n",
    "`get_links_from_wiki` function parses the article. It works by finding a div that contains the whole article, then iterates through all paragraphs and finds all links that match pattern `/wiki/article_name`. Because there's no domain in that pattern, it is added at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e1d066f-7404-4fe9-9035-e61d51541199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_from_wiki(soup, n=5, prefix=\"https://en.wikipedia.org\"):\n",
    "    \"\"\"\n",
    "    Extracts `n` first links from wikipedia articles and adds `prefix` to\n",
    "    internal links.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    soup : BeautifulSoup\n",
    "        Wikipedia page\n",
    "    n : int\n",
    "        Number of links to return\n",
    "    prefix : str, default=\"https://en.wikipedia.org\"\"\n",
    "        Site prefix\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of links\n",
    "    \"\"\"\n",
    "    arr = []\n",
    "\n",
    "    # Get div with article contents\n",
    "    div = soup.find(\"div\", class_=\"mw-parser-output\")\n",
    "\n",
    "    for element in div.find_all(\"p\") + div.find_all(\"ul\"):\n",
    "        # In each paragraph find all <a href=\"/wiki/article_name\"></a> and\n",
    "        # extract \"/wiki/article_name\"\n",
    "        for i, a in enumerate(element.find_all(\"a\", href=True)):\n",
    "            if len(arr) >= n:\n",
    "                break\n",
    "            if (\n",
    "                a[\"href\"].startswith(\"/wiki/\")\n",
    "                and len(a[\"href\"].split(\"/\")) == 3\n",
    "                and (\".\" not in a[\"href\"] and (\"(\" not in a[\"href\"]))\n",
    "            ):\n",
    "                arr.append(prefix + a[\"href\"])\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fc3e2b-ccc2-43c5-83b8-19bd9eab899b",
   "metadata": {},
   "source": [
    "The crawl function will be recursive, for each URL found on page I'll call it again. For rach iteration it'll check if URL contains that phrase, if so it'll return both the site and link to Philosophy. To control number of recursive calls, depth of created tree is limited by `depth` parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f8ce581-3521-408b-9422-fe1e378f8870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl(\n",
    "    pool: urllib3.PoolManager,\n",
    "    url,\n",
    "    phrase=None,\n",
    "    deep=1,\n",
    "    sleep_time=0.5,\n",
    "    n=5,\n",
    "    prefix=\"https://en.wikipedia.org\",\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Crawls given Wikipedia `url` (article) with max depth `deep`. For each page\n",
    "    extracts `n` urls and  if `phrase` is given check if `phrase` in urls.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pool : urllib3.PoolManager\n",
    "        Request pool\n",
    "    phrase : str\n",
    "        Phrase to search for in urls.\n",
    "    url : str\n",
    "        Link to wikipedia article\n",
    "    deep : int\n",
    "        Depth of crawl\n",
    "    sleep_time : float\n",
    "        Sleep time between requests.\n",
    "    n : int\n",
    "        Number of links to return\n",
    "    prefix : str, default=\"https://en.wikipedia.org\"\"\n",
    "        Site prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Tuple of url, list\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        site = url.split(\"/\")[-1]\n",
    "        print(f\"{deep} Entering {site}\")\n",
    "\n",
    "    # Sleep to avoid getting banned\n",
    "    time.sleep(sleep_time)\n",
    "    site = pool.request(\"GET\", url)\n",
    "    soup = BeautifulSoup(site.data, parser=\"lxml\")\n",
    "    \n",
    "    # Get links from wiki (I'll show it later)\n",
    "    links = get_links_from_wiki(soup=soup, n=n, prefix=prefix)\n",
    "    \n",
    "    # If phrase was given check if any of the links have it\n",
    "    is_phrase_present = any([phrase in link for link in links]) and phrase is not None\n",
    "    if deep > 0 and not is_phrase_present:\n",
    "        return (\n",
    "            url,\n",
    "            [\n",
    "                crawl(\n",
    "                    pool=pool,\n",
    "                    url=url_,\n",
    "                    phrase=phrase,\n",
    "                    deep=deep - 1,\n",
    "                    sleep_time=sleep_time,\n",
    "                    n=n,\n",
    "                    prefix=prefix,\n",
    "                    verbose=verbose,\n",
    "                )\n",
    "                for url_ in links\n",
    "            ],\n",
    "        )\n",
    "    return url, links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe1faa1-87b1-4a5b-b8d6-aec851d0af73",
   "metadata": {},
   "source": [
    "## The experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ea17ab0-ca9b-492d-9831-d14ee86506d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance of PoolManager that each crawler will share\n",
    "pool = urllib3.PoolManager() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea51c7d-9bfb-4251-afad-b77c49a4d405",
   "metadata": {},
   "source": [
    "To test the hypothesis we'll start from page `https://en.wikipedia.org/wiki/Data_mining\"`, look for page `Philosophy` and set link limit for crawler to `1` so that it'll only enter the first link on each page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de5c3d1c-089f-4b52-96c6-30261807a8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 Entering Doggart\n",
      "49 Entering Caroline_Doggart\n",
      "48 Entering Utrecht\n",
      "47 Entering Help:Pronunciation_respelling_key\n",
      "46 Entering Pronunciation_respelling_for_English\n",
      "45 Entering Pronunciation_respelling\n",
      "44 Entering Ad_hoc\n",
      "43 Entering List_of_Latin_phrases\n",
      "42 Entering English_language\n",
      "41 Entering West_Germanic_language\n",
      "40 Entering Germanic_languages\n",
      "39 Entering Indo-European_languages\n",
      "38 Entering Language_family\n",
      "37 Entering Language\n",
      "36 Entering Communication\n",
      "35 Entering Academic_discipline\n",
      "34 Entering Knowledge\n",
      "33 Entering Fact\n",
      "32 Entering Experience\n",
      "31 Entering Consciousness\n",
      "30 Entering Sentience\n",
      "29 Entering Emotion\n",
      "28 Entering Mental_state\n",
      "27 Entering Mind\n",
      "26 Entering Thought\n",
      "25 Entering Ideas\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('https://en.wikipedia.org/wiki/Doggart',\n",
       " [('https://en.wikipedia.org/wiki/Caroline_Doggart',\n",
       "   [('https://en.wikipedia.org/wiki/Utrecht',\n",
       "     [('https://en.wikipedia.org/wiki/Help:Pronunciation_respelling_key',\n",
       "       [('https://en.wikipedia.org/wiki/Pronunciation_respelling_for_English',\n",
       "         [('https://en.wikipedia.org/wiki/Pronunciation_respelling',\n",
       "           [('https://en.wikipedia.org/wiki/Ad_hoc',\n",
       "             [('https://en.wikipedia.org/wiki/List_of_Latin_phrases',\n",
       "               [('https://en.wikipedia.org/wiki/English_language',\n",
       "                 [('https://en.wikipedia.org/wiki/West_Germanic_language',\n",
       "                   [('https://en.wikipedia.org/wiki/Germanic_languages',\n",
       "                     [('https://en.wikipedia.org/wiki/Indo-European_languages',\n",
       "                       [('https://en.wikipedia.org/wiki/Language_family',\n",
       "                         [('https://en.wikipedia.org/wiki/Language',\n",
       "                           [('https://en.wikipedia.org/wiki/Communication',\n",
       "                             [('https://en.wikipedia.org/wiki/Academic_discipline',\n",
       "                               [('https://en.wikipedia.org/wiki/Knowledge',\n",
       "                                 [('https://en.wikipedia.org/wiki/Fact',\n",
       "                                   [('https://en.wikipedia.org/wiki/Experience',\n",
       "                                     [('https://en.wikipedia.org/wiki/Consciousness',\n",
       "                                       [('https://en.wikipedia.org/wiki/Sentience',\n",
       "                                         [('https://en.wikipedia.org/wiki/Emotion',\n",
       "                                           [('https://en.wikipedia.org/wiki/Mental_state',\n",
       "                                             [('https://en.wikipedia.org/wiki/Mind',\n",
       "                                               [('https://en.wikipedia.org/wiki/Thought',\n",
       "                                                 [('https://en.wikipedia.org/wiki/Ideas',\n",
       "                                                   ['https://en.wikipedia.org/wiki/Philosophy'])])])])])])])])])])])])])])])])])])])])])])])])])])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawl(pool, \"https://en.wikipedia.org/wiki/Doggart\", phrase=\"Philosophy\", deep=50, n=1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217c59bc-3558-48c8-a160-44fe931b9ee4",
   "metadata": {},
   "source": [
    "As you can see after 25 iterations indeed we found `Philosophy` page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd71fa76-0442-437c-afd2-55cb6bb8ce50",
   "metadata": {},
   "source": [
    "There's a famous Wikipedia phenomena that by clicking the first link in the main text of the article on the English Wikipedia, you'll eventually end up on the [philosophy](https://en.wikipedia.org/wiki/Philosophy) page. An explanation can be found [here](https://en.wikipedia.org/wiki/Wikipedia:Getting_to_Philosophy). Briefly, it's because of Wikipedia [Manual of Style guidelines](https://en.wikipedia.org/wiki/Wikipedia:MOSBEGIN) that recommend that articles begin by telling \"what or who the subject is, and often when and where\".\n",
    "\n",
    "This was true for roughly 97% of articles, so there's a big chance that by entering a random Wikipedia page and following the procedure you'll indeed end up on Philosophy. I could test this by hand, but this wouldn't be a dev.to article without writing some code. We'll start with how to download Wikipedia articles.\n",
    "\n",
    "## How to get data\n",
    "\n",
    "It's simple - just request contents of and article with `urllib3`. Wikipedia follows a convenient pattern for naming its articles. After the usual `en.wikipedia.org/` there's a `/wiki` and then `/article_name` (or media! we'll deal with that later) for example, `en.wikipedia.org/wiki/Data_mining`.\n",
    "\n",
    "Firstly, I'll create a pool from which I'll make requests to Wikipedia.\n",
    "\n",
    "```python\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "pool = urllib3.PoolManager() \n",
    "```\n",
    "\n",
    "From now on, I'll could download the articles one by one. To automate the process of crawling through the site, the crawler will be recursive. Each iteration of it will return `(current_url, [crawler for link_on site])`, the recursion will stop, at given depth. In the end, I'll end up with tree structure.\n",
    "\n",
    "```python\n",
    "def crawl(\n",
    "    pool: urllib3.PoolManager,\n",
    "    url,\n",
    "    phrase=None,\n",
    "    deep=1,\n",
    "    sleep_time=0.5,\n",
    "    n=5,\n",
    "    prefix=\"https://en.wikipedia.org\",\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Crawls given Wikipedia `url` (article) with max depth `deep`. For each page\n",
    "    extracts `n` urls and  if `phrase` is given check if `phrase` in urls.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pool : urllib3.PoolManager\n",
    "        Request pool\n",
    "    phrase : str\n",
    "        Phrase to search for in urls.\n",
    "    url : str\n",
    "        Link to wikipedia article\n",
    "    deep : int\n",
    "        Depth of crawl\n",
    "    sleep_time : float\n",
    "        Sleep time between requests.\n",
    "    n : int\n",
    "        Number of links to return\n",
    "    prefix : str, default=\"https://en.wikipedia.org\"\"\n",
    "        Site prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Tuple of url, list\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        site = url.split(\"/\")[-1]\n",
    "        print(f\"{deep} Entering {site}\")\n",
    "\n",
    "    # Sleep to avoid getting banned\n",
    "    time.sleep(sleep_time)\n",
    "    site = pool.request(\"GET\", url)\n",
    "    soup = BeautifulSoup(site.data, parser=\"lxml\")\n",
    "    \n",
    "    # Get links from wiki (I'll show it later)\n",
    "    links = get_links_from_wiki(soup=soup, n=n, prefix=prefix)\n",
    "    \n",
    "    # If phrase was given check if any of the links have it\n",
    "    is_phrase_present = any([phrase in link for link in links]) and phrase is not None\n",
    "    if deep > 0 and not is_phrase_present:\n",
    "        return (\n",
    "            url,\n",
    "            [\n",
    "                crawl(\n",
    "                    pool=pool,\n",
    "                    url=url_,\n",
    "                    phrase=phrase,\n",
    "                    deep=deep - 1,\n",
    "                    sleep_time=sleep_time,\n",
    "                    n=n,\n",
    "                    prefix=prefix,\n",
    "                    verbose=verbose,\n",
    "                )\n",
    "                for url_ in links\n",
    "            ],\n",
    "        )\n",
    "    return url, links\n",
    "```\n",
    "\n",
    "If you read the code carefully, you'd notice a function `get_links_from_wiki`. `get_links_from_wiki` function parses the article. It works by finding a div that contains the whole article, then iterates through all paragraphs (or lists) and finds all links that match pattern `/wiki/article_name`. Because there's no domain in that pattern, it is added at the end.\n",
    "\n",
    "```python\n",
    "def get_links_from_wiki(soup, n=5, prefix=\"https://en.wikipedia.org\"):\n",
    "    \"\"\"\n",
    "    Extracts `n` first links from wikipedia articles and adds `prefix` to\n",
    "    internal links.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    soup : BeautifulSoup\n",
    "        Wikipedia page\n",
    "    n : int\n",
    "        Number of links to return\n",
    "    prefix : str, default=\"https://en.wikipedia.org\"\"\n",
    "        Site prefix\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of links\n",
    "    \"\"\"\n",
    "    arr = []\n",
    "\n",
    "    # Get div with article contents\n",
    "    div = soup.find(\"div\", class_=\"mw-parser-output\")\n",
    "\n",
    "    for element in div.find_all(\"p\") + div.find_all(\"ul\"):\n",
    "        # In each paragraph find all <a href=\"/wiki/article_name\"></a> and\n",
    "        # extract \"/wiki/article_name\"\n",
    "        for i, a in enumerate(element.find_all(\"a\", href=True)):\n",
    "            if len(arr) >= n:\n",
    "                break\n",
    "            if (\n",
    "                a[\"href\"].startswith(\"/wiki/\")\n",
    "                and len(a[\"href\"].split(\"/\")) == 3\n",
    "                and (\".\" not in a[\"href\"] and (\"(\" not in a[\"href\"]))\n",
    "            ):\n",
    "                arr.append(prefix + a[\"href\"])\n",
    "    return arr\n",
    "```\n",
    "\n",
    "Now we have everything to check the phenomena. I'll set max depth to 50 and set `n=1` (to only expand first link in the article). \n",
    "\n",
    "```python\n",
    "crawl(pool, \"https://en.wikipedia.org/wiki/Doggart\", phrase=\"Philosophy\", deep=50, n=1, verbose=True)\n",
    "```\n",
    "\n",
    "Output:\n",
    "```output\n",
    "50 Entering Doggart\n",
    "49 Entering Caroline_Doggart\n",
    "48 Entering Utrecht\n",
    "47 Entering Help:Pronunciation_respelling_key\n",
    "...\n",
    "28 Entering Mental_state\n",
    "27 Entering Mind\n",
    "26 Entering Thought\n",
    "25 Entering Ideas\n",
    "\n",
    "('https://en.wikipedia.org/wiki/Doggart',\n",
    " [('https://en.wikipedia.org/wiki/Caroline_Doggart',\n",
    "   [('https://en.wikipedia.org/wiki/Utrecht',\n",
    "     [('https://en.wikipedia.org/wiki/Help:Pronunciation_respelling_key',\n",
    "       [('https://en.wikipedia.org/wiki/Pronunciation_respelling_for_English',\n",
    "\n",
    "...\n",
    "             [('https://en.wikipedia.org/wiki/Ideas',\n",
    "               ['https://en.wikipedia.org/wiki/Philosophy'])])])])])])])])])])])])])])])])])])])])])])])])])])\n",
    "```\n",
    "As you can see after 25 iterations indeed we found `Philosophy` page.\n",
    "\n",
    "Found this post interesting? Check out my [Github](https://github.com/finloop) @finloop\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
